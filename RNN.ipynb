{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 137773 chars, 81 unique\n"
     ]
    }
   ],
   "source": [
    "data = open('C:/Users/Parth/Desktop/Deep Learning/kafka.txt', 'r').read()\n",
    "\n",
    "chars = list(set(data)) \n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print ('data has %d chars, %d unique' % (data_size, vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n': 0, 'r': 1, '.': 2, 'a': 3, 's': 4, 'x': 5, '8': 6, 'X': 7, 'V': 8, 'c': 9, 'M': 10, 'W': 11, 'K': 12, 'e': 13, ':': 14, '$': 15, '©': 16, 'z': 17, 'o': 18, 'B': 19, 'R': 20, '!': 21, 'f': 22, 'ç': 23, 'J': 24, 'P': 25, 't': 26, 'u': 27, 'E': 28, '3': 29, 'k': 30, '6': 31, 'O': 32, 'i': 33, 'A': 34, 'S': 35, 'p': 36, '1': 37, 'v': 38, 'C': 39, 'T': 40, 'd': 41, '-': 42, '\\n': 43, 'b': 44, 'y': 45, '?': 46, 'Y': 47, 'Q': 48, '0': 49, 'F': 50, '@': 51, '2': 52, '*': 53, \"'\": 54, 'l': 55, 'H': 56, 'I': 57, 'N': 58, 'U': 59, 'h': 60, '(': 61, 'L': 62, '9': 63, 'D': 64, '5': 65, 'q': 66, 'j': 67, '4': 68, 'G': 69, '/': 70, 'w': 71, 'g': 72, ' ': 73, '\"': 74, '7': 75, '%': 76, 'm': 77, ')': 78, ',': 79, ';': 80}\n",
      "{0: 'n', 1: 'r', 2: '.', 3: 'a', 4: 's', 5: 'x', 6: '8', 7: 'X', 8: 'V', 9: 'c', 10: 'M', 11: 'W', 12: 'K', 13: 'e', 14: ':', 15: '$', 16: '©', 17: 'z', 18: 'o', 19: 'B', 20: 'R', 21: '!', 22: 'f', 23: 'ç', 24: 'J', 25: 'P', 26: 't', 27: 'u', 28: 'E', 29: '3', 30: 'k', 31: '6', 32: 'O', 33: 'i', 34: 'A', 35: 'S', 36: 'p', 37: '1', 38: 'v', 39: 'C', 40: 'T', 41: 'd', 42: '-', 43: '\\n', 44: 'b', 45: 'y', 46: '?', 47: 'Y', 48: 'Q', 49: '0', 50: 'F', 51: '@', 52: '2', 53: '*', 54: \"'\", 55: 'l', 56: 'H', 57: 'I', 58: 'N', 59: 'U', 60: 'h', 61: '(', 62: 'L', 63: '9', 64: 'D', 65: '5', 66: 'q', 67: 'j', 68: '4', 69: 'G', 70: '/', 71: 'w', 72: 'g', 73: ' ', 74: '\"', 75: '7', 76: '%', 77: 'm', 78: ')', 79: ',', 80: ';'}\n"
     ]
    }
   ],
   "source": [
    "char_to_ix = { ch:i for i,ch in enumerate(chars)}\n",
    "ix_to_char = { i:ch for i, ch in enumerate(chars)}\n",
    "print (char_to_ix)\n",
    "print (ix_to_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "vector_for_char_a = np.zeros((vocab_size, 1))\n",
    "vector_for_char_a[char_to_ix['a']] = 1\n",
    "print (vector_for_char_a.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#hyperparameters\n",
    "hidden_size = 100\n",
    "seq_length = 25\n",
    "learning_rate = 1e-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#model parameters\n",
    "\n",
    "Wxh = np.random.randn(hidden_size, vocab_size) * 0.01 #input to hidden\n",
    "Whh = np.random.randn(hidden_size, hidden_size) * 0.01 #hidden to hidden\n",
    "Why = np.random.randn(vocab_size, hidden_size) * 0.01 #hidden to output\n",
    "bh = np.zeros((hidden_size, 1)) #bias to hidden\n",
    "by = np.zeros((vocab_size, 1)) #bias to output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def lossFun(inputs, targets, hprev):\n",
    "#store our inputs, hidden states, outputs, and probability values\n",
    "    xs, hs, ys, ps, = {}, {}, {}, {} #Empty dicts\n",
    "    # Each of these are going to be SEQ_LENGTH(Here 25) long dicts i.e. 1 vector per time(seq) step\n",
    "    # xs will store 1 hot encoded input characters for each of 25 time steps (26, 25 times)\n",
    "    # hs will store hidden state outputs for 25 time steps (100, 25 times)) plus a -1 indexed initial state\n",
    "    # to calculate the hidden state at t = 0\n",
    "    # ys will store targets i.e. expected outputs for 25 times (26, 25 times), unnormalized probabs\n",
    "    # ps will take the ys and convert them to normalized probab for chars\n",
    "    # We could have used lists BUT we need an entry with -1 to calc the 0th hidden layer\n",
    "    # -1 as  a list index would wrap around to the final element\n",
    "    xs, hs, ys, ps = {}, {}, {}, {}\n",
    "      #init with previous hidden state\n",
    "    # Using \"=\" would create a reference, this creates a whole separate copy\n",
    "    # We don't want hs[-1] to automatically change if hprev is changed\n",
    "    hs[-1] = np.copy(hprev)\n",
    "      #init loss as 0\n",
    "    loss = 0\n",
    "    # forward pass                                                                                                                                                                              \n",
    "    for t in range(len(inputs)):\n",
    "        xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation (we place a 0 vector as the t-th input)                                                                                                                     \n",
    "        xs[t][inputs[t]] = 1 # Inside that t-th input we use the integer in \"inputs\" list to  set the correct\n",
    "        hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state                                                                                                            \n",
    "        ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars                                                                                                           \n",
    "        ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars                                                                                                              \n",
    "        loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)                                                                                                                       \n",
    "    # backward pass: compute gradients going backwards    \n",
    "    #initalize vectors for gradient values for each set of weights \n",
    "    dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "    dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "    dhnext = np.zeros_like(hs[0])\n",
    "    for t in reversed(range(len(inputs))):\n",
    "    #output probabilities\n",
    "        dy = np.copy(ps[t])\n",
    "    #derive our first gradient\n",
    "        dy[targets[t]] -= 1 # backprop into y  \n",
    "    #compute output gradient -  output times hidden states transpose\n",
    "    #When we apply the transpose weight matrix,  \n",
    "    #we can think intuitively of this as moving the error backward\n",
    "    #through the network, giving us some sort of measure of the error \n",
    "    #at the output of the lth layer. \n",
    "    #output gradient\n",
    "        dWhy += np.dot(dy, hs[t].T)\n",
    "    #derivative of output bias\n",
    "        dby += dy\n",
    "    #backpropagate!\n",
    "        dh = np.dot(Why.T, dy) + dhnext # backprop into h                                                                                                                                         \n",
    "        dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity                                                                                                                     \n",
    "        dbh += dhraw #derivative of hidden bias\n",
    "        dWxh += np.dot(dhraw, xs[t].T) #derivative of input to hidden layer weight\n",
    "        dWhh += np.dot(dhraw, hs[t-1].T) #derivative of hidden layer to hidden layer weight\n",
    "        dhnext = np.dot(Whh.T, dhraw) \n",
    "    for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "        np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients                                                                                                                 \n",
    "    return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      " çN5m't,R/Mc1;\n",
      "kP$9@;tFuçUhCmlTv':i\n",
      "nCdfo%UrUSsx)* Ce?-wm6yd/Iu1Ew\n",
      "t©nyDgKfTdXapKGixUYOjd0Ti©k'@,:Eyç$©7Q$KdTDD,4Vv%xeBk9pbK(WTRRWr8gl3çh;çGlT(@ML?eEe1n8,8uUxw/rLiJ:oSQ@Jc68)©X)RpJb.%f3@@rp:M.4©6\n",
      "HUs1\" \n",
      "----\n"
     ]
    }
   ],
   "source": [
    "#prediction, one full forward pass\n",
    "def sample(h, seed_ix, n):\n",
    "    #create vector\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    #customize it for our seed char\n",
    "    x[seed_ix] = 1\n",
    "    #list to store generated chars\n",
    "    ixes = []\n",
    "    #for as many characters as we want to generate\n",
    "    for t in range(n):\n",
    "    #a hidden state at a given time step is a function \n",
    "    #of the input at the same time step modified by a weight matrix \n",
    "    #added to the hidden state of the previous time step \n",
    "    #multiplied by its own hidden state to hidden state matrix.\n",
    "        h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
    "    #compute output (unnormalised)\n",
    "        y = np.dot(Why, h) + by\n",
    "    ## probabilities for next chars\n",
    "        p = np.exp(y) / np.sum(np.exp(y))\n",
    "    #pick one with the highest probability \n",
    "        ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "    #create a vector\n",
    "        x = np.zeros((vocab_size, 1))\n",
    "    #customize it for the predicted char\n",
    "        x[ix] = 1\n",
    "    #add it to the list\n",
    "        ixes.append(ix)\n",
    "\n",
    "    txt = ''.join(ix_to_char[ix] for ix in ixes)\n",
    "    print ('----\\n %s \\n----' % (txt, ))\n",
    "hprev = np.zeros((hidden_size,1)) # reset RNN memory  \n",
    "#predict the 200 next characters given 'a'\n",
    "sample(hprev,char_to_ix['a'],200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs [32, 0, 13, 73, 77, 18, 1, 0, 33, 0, 72, 79, 73, 71, 60, 13, 0, 73, 69, 1, 13, 72, 18, 1, 73]\n",
      "targets [0, 13, 73, 77, 18, 1, 0, 33, 0, 72, 79, 73, 71, 60, 13, 0, 73, 69, 1, 13, 72, 18, 1, 73, 35]\n"
     ]
    }
   ],
   "source": [
    "p=0  \n",
    "inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "print (\"inputs\", inputs)\n",
    "targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "print (\"targets\", targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0, loss: 109.861224\n",
      "----\n",
      " \n",
      "/.aQçlM:-e-.X!?Pçmi/QMztyasH-/ldCieao-ç(j%$@STcsBSç61m45q6D\"vPDN03\n",
      ".jeIKl?w\"A!iGntC/)Xa\"jumxfF?S(!?)MkGO9:vz'I5E©8moYP;NjOkkJ6(C3NGbM,a3w3W:Nl'uUz09QILTz7'F7ezDVAJgSXHr$jk:/IC'Lp;pT;cu*LF3;çld ;kcDgJ \n",
      "----\n",
      "iter 1000, loss: 84.013698\n",
      "----\n",
      " , amlree veithim he tis nik re cl ane, he fnwiecey fhe fiswke he difs. anth, soin an wate the chiaf he dore ge semd chke ritcive ke he cant mny viscenk le te tor napt thal ils, ee the ind cer hiserout \n",
      "----\n",
      "iter 2000, loss: 66.116330\n",
      "----\n",
      " puld ind sieg d aj here ltoovere his hing, if cghe ale ag he whar his leameare couthemhe that mo he corackithurerer ser aveneay. meimuster hish le'cacimous hiske qus she co mome he whithlathe the ou h \n",
      "----\n",
      "iter 3000, loss: 56.892048\n",
      "----\n",
      " d:cither sand, the there ulson tared sved st und gomling\"nbithece hing nody aid ond, pelly aryey onct; ttaklver the covererey; ngnurly y she m\" er the snonly oudse therd folwmund roihered arent  kers  \n",
      "----\n",
      "iter 4000, loss: 52.256488\n",
      "----\n",
      "  hims way sat oukhed fike. pbanck it race sith chard cos he hipt Gregor he cat Gregot rome in the mislaswwang beiny to wible ait aine oowwatll, Gregot kiok at phe had a kifkighing her at harro thitht  \n",
      "----\n",
      "iter 5000, loss: 56.272700\n",
      "----\n",
      " - putenghe, iod ip\n",
      " horenonperot syicoo te tr ppanin.\n",
      " any, cor angra1 (he\"1\n",
      " tgre apreorre worisadipkiw cougesnes whsa ondidececwiibote Louyed1sy anukade armsancid Ufwres boreryiig.\n",
      "\n",
      "B\n",
      "1atate errin o \n",
      "----\n",
      "iter 6000, loss: 59.053204\n",
      "----\n",
      " se the but in; sithorle thin cored ar in eaace ofsan wo co com wame one lo wout san whe Oo wat ard toov prawm was ting to cotin wad all bass- shes ese to il ro-. Ho whapl har vouy sund os he was be eo \n",
      "----\n",
      "iter 7000, loss: 53.006754\n",
      "----\n",
      " . Gregarls a had,ers farage outsorejover had to ns of out sound chith Gregor. Irongod the rasthis iigime thet alin overwest were half rira wathhang, egough wlow; there we dore is asud Grogauf wat anti \n",
      "----\n",
      "iter 8000, loss: 48.857288\n",
      "----\n",
      "  yom ho  has frojed gobl abd siy nowte fieges thape hlanto that -rom she thatsed all heatetpasappten bet now hamiss bee wsoscly boly cought, gund; aBs to strecess Grecurly she lo him would calliclle s \n",
      "----\n",
      "iter 9000, loss: 47.084327\n",
      "----\n",
      "  ap intald suel had nohe wost bughc hes couss ove sthe wesche nos withe him spom becharpthing roure, rkito him, his nomong pabley his mosing bingm n0tra.e vprist righ, overf spre as allulinr arrecting \n",
      "----\n",
      "iter 10000, loss: 46.440235\n",
      "----\n",
      " ing if back \"-y dwingate non for stying bratee, nony. Theyr a math had on the coureec his nom to mt. Sand becios wor ftapen lees, Gregor old sisp whee. Se terl in spme and to forked to brer tuit up hi \n",
      "----\n",
      "iter 11000, loss: 53.496663\n",
      "----\n",
      " ld.E. ming comere conke il the drot\n",
      "cut alleblacefoisten a way dotl ovo Gron the Grecard and Kace and eveas busink.\n",
      ". neben, Fout intensly and the catan. mayaching was puthec tw-anden, to 50ror sise.  \n",
      "----\n",
      "iter 12000, loss: 50.040998\n",
      "----\n",
      " ert. rad whein? morles whire iectent aid gotmille com otss terf. Held, on opergope dooulled seor .w selagide, the ghat thould sfers toreld to eoted and thad wif of the dook that that deazthes Ifiof th \n",
      "----\n",
      "iter 13000, loss: 46.580963\n",
      "----\n",
      " e fore a aither ne loch would do Gregor caring them and shoul easter for'm fortacely I blat erattey on him fars brow slring wear arly a a he his fan he wlarss arate sar sid a his dore ple from strould \n",
      "----\n",
      "iter 14000, loss: 44.564789\n",
      "----\n",
      "  sas wather the finde Greging wiof that had not the a the trow; ragdisine her waund waciobsened anaofing leong; bme was the had be so him; -erichingif Gregor undt he way had doly allion to aby cather, \n",
      "----\n",
      "iter 15000, loss: 44.156512\n",
      "----\n",
      " l bet on Gregor of hears, ince ie unainn come and and tiat to alven wemeat and they 0fole anat. Her, fropll the hell tullemone conte. He comestterdinge itly doted ar a corest onded to lithech, se snem \n",
      "----\n",
      "iter 16000, loss: 46.892835\n",
      "----\n",
      " sing usay bubitsing int2ice cast, the workss bection. wrojitewargengen reciaseat, as corest exgruge minde qugereds eotmatemed and aKy wertion merkerively Projectirgode by ne with acE wous the Projes w \n",
      "----\n",
      "iter 17000, loss: 49.096217\n",
      "----\n",
      "  quid pordated bagond, enat out and not he  dear. All to lack blagle mo mothen. Harsa drould to nstemppex sines be by thaik of a lenlary fod ho sintlle exund a  sideem spey on a comvingeby voully azou \n",
      "----\n",
      "iter 18000, loss: 46.023871\n",
      "----\n",
      "  nons or to haand ir. In tans, bebagly thas had a his fast nad he loot sain the contuldcedtan he waMd had it to had slate whalick. Sar atly that; and him on itpllowne he way now Gregou hiss; as the to \n",
      "----\n",
      "iter 19000, loss: 43.719384\n",
      "----\n",
      " ny donbpor't dood ontentied wont; on quing real and by eney frow leficelly ary?\n",
      " hist, peraperten fable whiter that he latar of reg's ink. Ospingt of thound aspent dad he could jeass very that in towa \n",
      "----\n",
      "iter 20000, loss: 43.041143\n",
      "----\n",
      " ern torrate letse; wathough. stey wool drouglythit Gregor brent the tayn was to firn oritsiof hir thourture slojed hersith beestaning reous youm and juch have, in didnethirgongs. Ad exaingor, soor lit \n",
      "----\n",
      "iter 21000, loss: 42.859128\n",
      "----\n",
      " e cabanbone besm lat it any ack then them and the all witheling fron sloce beatluch tuld the lefkre and now bet sowing was went buse wathont only wlotew with act seiced in. So jut f mather to lake tha \n",
      "----\n",
      "iter 22000, loss: 48.234541\n",
      "----\n",
      " ren asture ofct: couporteborned horino Pwait. the Fayect.\n",
      "\n",
      "WSok\n",
      "3ay Untser Ancpare aicermagled\n",
      "and muricfuns thatsatidinss. ERvonids:vs pate that we0ceraned, andous Arcing on at open55\" on reainger, d \n",
      "----\n",
      "iter 23000, loss: 46.275170\n",
      "----\n",
      " r hel as theme in wand.  Yay and laic excecemsiout he with inbousuling8 sapely with undiimics imso has nojurent enmaden quiaglites thing with the would made a koollyingation, and ever tar afaridat, al \n",
      "----\n",
      "iter 24000, loss: 43.559602\n",
      "----\n",
      " chem elounded encere't soom about isted the ome for's unieeing tuar been shews.\n",
      "\n",
      "1.chay your's dist. 8ably vemy of they at ally passe whst, and noct., he inge atter his to beso movichaiss, wich of doo \n",
      "----\n",
      "iter 25000, loss: 41.978232\n",
      "----\n",
      " speased mett, shill wat, awlyand ins) as what lay. The to go mading mave as in her away sases the would and posise mritent at her incariacy wam and from the dive sture all now arrettowing had soor, wo \n",
      "----\n",
      "iter 26000, loss: 41.868552\n",
      "----\n",
      " fabsed\n",
      "with the rookberedone could ad for shomesunt arrunghn thes win way acling elmay comasing ongrone his sisters swres a mualy; the rigt was by oopling; that it ffark. Hess beenonn wer to be that e \n",
      "----\n",
      "iter 27000, loss: 43.975583\n",
      "----\n",
      " couthed with grop\n",
      "ly of on out any vering\n",
      "ccuppiegs in they not itllion to on. Obon the bion plegr sull\n",
      "now clecine dabled and pound.\n",
      "\n",
      "Chise work to onenge1.\n",
      "\n",
      "\n",
      "The Projectras uprent.\n",
      "\n",
      "Gregor to se tea \n",
      "----\n",
      "iter 28000, loss: 46.315877\n",
      "----\n",
      " unt the\n",
      "hat rece to keal condobergecld theeftrov which and sgate-tere was hork this bals at this warcontientenurleth. STong neatee horn gix lad their inso erees to peront oevid. The would as te lees h \n",
      "----\n",
      "iter 29000, loss: 43.831423\n",
      "----\n",
      " ears And frampring you how his the clat leftherned tomees tranes., of the lalnt po moke foary, this way caine see, aike on and jupbyrabld fout a lighted intappanaise any wintcs, say lyoor bees quated  \n",
      "----\n",
      "iter 30000, loss: 41.897661\n",
      "----\n",
      " ing her. In his tastiving his ret of ceuring that thimath condess. She didnond and comtid and wintalf be wourd no in they the and and room uads able wpomelf ley of at in me the mureed octer that, expl \n",
      "----\n",
      "iter 31000, loss: 41.426845\n",
      "----\n",
      " e henmousissed but the had of there to had mat, he mact all sister bees thinge him soke bashle wortiofing had himsound batent plecrest left notelo here ove of asterith his - and to sopy nem, alliegs h \n",
      "----\n",
      "iter 32000, loss: 41.264149\n",
      "----\n",
      " veny nombat., saod. I'd reave, nont to but they and, it. jusppy steber. She the camen\" what as ep and hearly sthis for hhaco withth her homong alconday I could forslad diquted if whan. Fould bemonts i \n",
      "----\n",
      "iter 33000, loss: 45.729209\n",
      "----\n",
      " s le swor a paigly fetention here siin The to do reveriner's rafork intous of roiget of compunterg: \n",
      "Gren comsed founctatecobysurre IN tagat to noms the mother chectref precurr\n",
      "state: 2cumet\n",
      "prenem.FN \n",
      "----\n",
      "iter 34000, loss: 44.426898\n",
      "----\n",
      " reced; and catsed ally. beforidy verbing a chilk.\n",
      "\n",
      "They $reworst now turing; he hassw. \"9e to befbed to adapstinon's would get oncoully would stam have that erepor's frenily furs, cound lokm, Aroow- a \n",
      "----\n",
      "iter 35000, loss: 42.039914\n",
      "----\n",
      " t from exchites, wfalt. Gregor's be't deced teyts, ofted hacksing, well when he had, but the wiil streedly feft that the dlo-chude the fiddenicly besent asaickstor withou drow a.d of co himself sofe5t \n",
      "----\n",
      "iter 36000, loss: 40.653205\n",
      "----\n",
      " at that his forteb here thaa sintor momished bechery had with very al ad that in and thatrite, and us at a mat's shise the win of where inde must up again to to with he cay hep that if hisself that Gu \n",
      "----\n",
      "iter 37000, loss: 40.521539\n",
      "----\n",
      " hempel his alporising the callars out beloccery perker thour us that out - he nochndy faps oncest his was stat ofere coppus lay. Weart turined frospesor what had it for tumened fior bything him see th \n",
      "----\n",
      "iter 38000, loss: 42.416537\n",
      "----\n",
      " ove\n",
      "\n",
      "\n",
      "ploted csasior and in the turreling for to were jughn the thos greateny evers.\n",
      "\n",
      "AnEte wided be to clay every, with the clay of the grounric turress - I.1\n",
      "Projlet.\n",
      "\n",
      "The susistery the\n",
      "rofte berant \n",
      "----\n",
      "iter 39000, loss: 44.718398\n",
      "----\n",
      " oor hirciecen, an they groutecwloug not lease of de him fave bord weased the vork at he whatceres very sould to gith thouy of though; corde or wiof\n",
      "vord swors hear and dick he notly at had any, posper \n",
      "----\n",
      "iter 40000, loss: 42.515057\n",
      "----\n",
      " ee; afthink camp he wnyeusinese dome of he backing, and siments and paally in.  Grom his door awat himtout motha a wither, of everstrom fot make, he cesmsing, at as indond onazingorise, browmed whough \n",
      "----\n",
      "iter 41000, loss: 40.733628\n",
      "----\n",
      "  near whalessing leoln dis unllaly iver abote helly jawalcesuld was hes. Gregen's father his foon's had atters that; and the mush afalfed and all all round ugro get a mored entige his roulvers foller  \n",
      "----\n",
      "iter 42000, loss: 40.406102\n",
      "----\n",
      " er the stide the dang of able to no necinging const atemhiorwemst of he rushed ly hem they shiss, diffiee to ceachene fom who hadd theiey unowhetsien, and shome was quicing panting opeef as his serm h \n",
      "----\n",
      "iter 43000, loss: 40.235577\n",
      "----\n",
      " sed lyen turw? of not in the upite to becary. AstanninC, to warred and fragastsure, and doon, tonard the bundagh sherrell about little the ead sime a keeved like gave pead lyined\", bentred into reild  \n",
      "----\n",
      "iter 44000, loss: 44.115283\n",
      "----\n",
      " hatserk jusing speatenice Mr.  tas lans.\n",
      "\n",
      "\n",
      "\n",
      "D contled of as if witictuned in a might unct this di-pield the work Sull to or offectime puronems ofele a.c.\n",
      "SL wiss ferldin Mrceme proppiit Project of arr \n",
      "----\n",
      "iter 45000, loss: 43.277025\n",
      "----\n",
      "  father he s int in the chilk wect the it hindy was was one out is thalk!\" The\n",
      "r if his itching inso the lock wasting, all, Guthent a wistnte Drow! He wandstathing incil!\" Deld. That's shohing he wast \n",
      "----\n",
      "iter 46000, loss: 41.001303\n",
      "----\n",
      " wort foln eomat. Gregor's to the clewsided he may as himseef.\"L. Os way door ones to seed coslatened and Gregor as the rougot for sthe feel actore the foold tive you coulatase to to expy pund that dis \n",
      "----\n",
      "iter 47000, loss: 39.826449\n",
      "----\n",
      " at inttemmifused cenen taviste wet of his file drames hermed works him nothe fally in was him liggly scout himsly himfer in sid popy work. stiad with herwadsien it. Thaid ope- as would went deavin. Sh \n",
      "----\n",
      "iter 48000, loss: 39.699081\n",
      "----\n",
      " rnay ftre coup forcwager't shoumslated back the wible not she could hed the close would comaitcedioch realf. Gregor's lay was one moggfolf looke had to an oper: Thet nothaning his of him. Ond withuup  \n",
      "----\n",
      "iter 49000, loss: 41.322143\n",
      "----\n",
      " oveade workmiick ceremeso cat a sicaundd ant ul and and Project Geqe up and rent parctrying farmate move, waproonbed lyitua bectan by do what fremted if ensins if his wiet Pretiomy\n",
      "widcems iits in the \n",
      "----\n",
      "iter 50000, loss: 43.698995\n",
      "----\n",
      "  no without in a deaded were tryin close, buck!bergurss the fall like to calcoundents bleapud was only, and not to bide the and wonser ant about they froht work the Foolf to het tra with comamlned wit \n",
      "----\n",
      "iter 51000, loss: 41.643440\n",
      "----\n",
      " nading atech.  ve the cord wire rops desr you the sher thempous a spe's ouldlyivs surfed on't slow that.\n",
      "\n",
      "The chor moster alread to of Gren apread. The him, buck expying it hersant in crosl, home whis \n",
      "----\n",
      "iter 52000, loss: 39.913139\n",
      "----\n",
      " eal then his weefome freive tow, bedaysey they wirtet the centurr sinns, aw of the kelp move aspatenssone his was the operms wholn that in thith cand nos be thouctent, it as to this cleeph, where, be  \n",
      "----\n",
      "iter 53000, loss: 39.767524\n",
      "----\n",
      " cem who would surter; therestiin threally a mady the'stensenly, the bus sce, and have fillpevine of as, and closed farcen begore he befo fealf toont of this is his day, was of aly bidning lath it havi \n",
      "----\n",
      "iter 54000, loss: 39.521856\n",
      "----\n",
      " t hems leap, hersing of the word using ganded her out his doors every. He was the vome in back in the readtel. Mrss quinged the cheewing addeving see of asprech, laye her afathen on cos, that no been  \n",
      "----\n",
      "iter 55000, loss: 42.990764\n",
      "----\n",
      "  D5iedieefise dermyed and the andro most a ccouch.\n",
      "\n",
      "Forot or do grom\n",
      "perawnbent. FOved and fremss the Project Gutenberg-tm Sas rece\n",
      "went formate reppible ses for alraitissave riglecter witheace bying  \n",
      "----\n",
      "iter 56000, loss: 42.404843\n",
      "----\n",
      " sing terat workenss it ormesthing  now the \"I\" 1 on up thurk whealy moan queass cormatain wich. I's gear and gotw att one how that other there, came and nexpops in echer of the pared him rent incest o \n",
      "----\n",
      "iter 57000, loss: 40.362705\n",
      "----\n",
      " withet and about and lead and sicked about heard a and have leamien and bep operys. But thee; herce the fad to ho dared sure she fadinge preaped other all in about of the antached of his dith. Othow h \n",
      "----\n",
      "iter 58000, loss: 39.173851\n",
      "----\n",
      " ead gaver hardemsiceestibitfed while compry? goming to fill. IMy with whole; mare him I mace, agreack, him tornint.  the room room tirer. , so unctard had sllown there and there she make the lotlen an \n",
      "----\n",
      "iter 59000, loss: 39.004124\n",
      "----\n",
      " n as their only thouk father overilly booiten hivever soon onte from the moven falloottitmousn, put mulv. IMy thied forat ongo the chace bet tizen. Are a long, impsuadiny in the diduneestrespey way; a \n",
      "----\n",
      "iter 60000, loss: 40.653098\n",
      "----\n",
      "  mo-bers fet say with arre fertancemest\", chain to tin to clyation. B0My words poppridoon the wad not Lelate, for too.PWed hominger\", mint Rojecthaite bighon. Where ut antores, with as or atrive jeet. \n",
      "----\n",
      "iter 61000, loss: 42.961583\n",
      "----\n",
      " icwicla!s all this bust cpote, he had ferter, - this, itt tin agaeeing him jurelf undint abately left lief chall or that of notllile very his budingd - fome, belk the regary with and was the dools, he \n",
      "----\n",
      "iter 62000, loss: 40.996772\n",
      "----\n",
      " w you reonic and the pacing the the fro-s. L's hand thing the barn neldow. His lefully vag atthen at the cwere the choot'y ostire seard chece neels his kleinly bense. work, the fiste a chilk. The e.;  \n",
      "----\n",
      "iter 63000, loss: 39.354557\n",
      "----\n",
      " , sinded were all.\n",
      "\n",
      "EMclethe was to sevened a wap and hel as opened uncele thaw would bering ie him. BU swoubly. \"Itroun be det. Welledangungss at it drom. It. FIRccess lithen and non would sas stomen \n",
      "----\n",
      "iter 64000, loss: 39.137342\n",
      "----\n",
      " Gregor and, it west that shing that Gregor it unote it feel bamievion leed himserk and rearly fot was char lecemote to wroftly was mother nears the rabs hour sladally.\n",
      "\n",
      "All able bring. He buadenmaytat \n",
      "----\n",
      "iter 65000, loss: 38.943806\n",
      "----\n",
      " ntwadiple in the door. Her. Not\n",
      ".h. Whimangungiceard and cuther what Iud waschally distee, knous untilong. he comy in even the opewands and out onat cout up everys tound remat to slerus that even allm \n",
      "----\n",
      "iter 66000, loss: 42.090770\n",
      "----\n",
      "  Praye Found\n",
      "1.F ANTHK be the co mave and alentenend fANEn PFo/k you the grke (oul be a\n",
      "Fedion: That light ome - and the were forid se the\n",
      "preat pan full anr Gutemberg the 5's Prou(d to ne siminest of \n",
      "----\n",
      "iter 67000, loss: 41.767487\n",
      "----\n",
      " e now not as his fust night ofstat oth his reerlused wirmemoutly in any his consayiboth the browly then's sistercestinming onctide, the worro him. I my the pover their jo theon a heles in weally mod b \n",
      "----\n",
      "iter 68000, loss: 39.880979\n",
      "----\n",
      " abled - mime his sister which fermal in his reap list couning any furgat of his plothels ady soke had roowassed the, his lofe acchingly at mo goll thatcercis wall the lofaren be enso of this bucare in \n",
      "----\n",
      "iter 69000, loss: 38.652525\n",
      "----\n",
      " murent win; it woilus, as he could becariby, ,ou macun had to rearnene alracked ougion her to the condowioning on the dooll was clenself nork whnt; with it's puked; Greter. It allye tranrend, the deme \n",
      "----\n",
      "iter 70000, loss: 38.494629\n",
      "----\n",
      " uriinpiling attiened see dote cley day and not over to gent stat, and uping about him thish, and alrimast that had of trom as speat aut to it evonis. Abdong to that mo-nself of colly to the blan on tu \n",
      "----\n",
      "iter 71000, loss: 40.046583\n",
      "----\n",
      " Propection work (0E S000U RE8 PBys ully to backly presuriotiontoreaks\n",
      "\n",
      "PLocerghe dry wlece you disingerarmections with hing the otcingory Project naitallt.\n",
      "\n",
      "The Perstrrethertotent of crostratayeven qu \n",
      "----\n",
      "iter 72000, loss: 42.387865\n",
      "----\n",
      " over hel. OR of his most ele\", Gregor enavemes that as Gregor capsudiluy a met thize door, theiviould ghough, be withenorestirned the the coat have 3aske, out work!\", Gucele my wainse clecked of there \n",
      "----\n",
      "iter 73000, loss: 40.477407\n",
      "----\n",
      " n for did had hime and and san ontiente days it she town and, who were of the the tame that I were, leven of the condout and in. And we the couldn to the openbouped his molf thee haw prought have al w \n",
      "----\n",
      "iter 74000, loss: 38.929750\n",
      "----\n",
      " ingom for ages to eloughtardabld incy.\n",
      " When his monee agre he had room found her back sise where him to parntation but shis soin quike of the that hes comitte to now to the word shou body when his fo \n",
      "----\n",
      "iter 75000, loss: 38.746728\n",
      "----\n",
      "  with umped streened for dise with the pusped or her I'm to maitied his gind a windy turn from wither rownst uneclee them able they boonv then?S\"\" Have when the wese her comistay hip as in it he for G \n",
      "----\n",
      "iter 76000, loss: 38.495738\n",
      "----\n",
      " ad shew doorgen no eliugherstard. \"Whish whatien; hrind as went know up And so him and the diak all of a who go the are that was ont took dawnded undon ablarked come did left much gent stiet. It the f \n",
      "----\n",
      "iter 77000, loss: 41.116744\n",
      "----\n",
      " oumented as and caranftsagearmend, putmer armand and doinct dask, sirase all or the poffecesny arwarmay dearemprely to sistor sidisedobborgith qualitaters\n",
      "and the esplakn\n",
      "5512AV2.  OR\n",
      "E Project ance o \n",
      "----\n",
      "iter 78000, loss: 41.226167\n",
      "----\n",
      " oss threes he himsear. I'ld vent wont of the head ops up day alouted ton's condliticenty rambergor of twening\", he cant. But resprated to gealy to at him al the didister his seiving to shouss into hem \n",
      "----\n",
      "iter 79000, loss: 39.430914\n",
      "----\n",
      " liked. Gregor as all posurecten alagened, Grege - as he was bocmanger dramset o? to shims. \"Mother alrlaray agnering she made of has sooms, antront his father whet her to sindist of coming a leake an  \n",
      "----\n",
      "iter 80000, loss: 38.268470\n",
      "----\n",
      "  to ot hes a\" the bome - hered the room as anything with out of his door kefy for still up comy. \"I stracil not to him distooferarcaut that vome sood bot theees the flosld so about some, so cuntevut i \n",
      "----\n",
      "iter 81000, loss: 38.126137\n",
      "----\n",
      " readionly room agring in and room leadly had ceathonced Gregor indone would evount've he hnerpeganelic noticenbermeat:\n",
      " project mskers, Gregitherlagly siffed. Oneap ouchersicing it had in evened abaut \n",
      "----\n",
      "iter 82000, loss: 39.614367\n",
      "----\n",
      " pounding to baniemest if soring be sefforientel the beca\n",
      "\n",
      "as tiling strite\n",
      "prolnor! So\n",
      "\n",
      "\n",
      "\n",
      "*1\n",
      "PUnE dout ofter him rearentes at ary. \n",
      "He sibcon, do gelly sound undle not or adiyob's - the pruthe to efec \n",
      "----\n",
      "iter 83000, loss: 41.924316\n",
      "----\n",
      " w sholes quis, soon, \"gole have He sly possint the lonker. I, star said all\", saidy meblly and his dis not becaieslybable outly dable. The, he had copsly not Greg his and grla, which his prooto the ca \n",
      "----\n",
      "iter 84000, loss: 40.114170\n",
      "----\n",
      " ergonin carts that forchawnom mest up head, are had donentemps not youd. He wese agureca convening lat the kize conviee and the m'll his ome he derled when that deaks: regorsto felenes of after to pew \n",
      "----\n",
      "iter 85000, loss: 38.606810\n",
      "----\n",
      " ler his notring ie the to to to butsed and fleeptionly aspliceder to lock out, Mrate she lainge torwor three that had that it, all thistere he hope out voll cotel, it of cound in imerations it had wit \n",
      "----\n",
      "iter 86000, loss: 38.386461\n",
      "----\n",
      " r thair\n",
      "that thad have to privet be taace simasel way sounerable in time under this rentongs and his fast of the romepchitus stayes, beconely work would fordowed to silk, undibhen foops to tracuecht w \n",
      "----\n",
      "iter 87000, loss: 38.146809\n",
      "----\n",
      " ece clathed, kent the k oots he could said Isfays of cound his sister to reakse. \n",
      "ake was owabling was himstinit of that wase do, and into resneary cake to reytenring; whilucining sister convinee magh \n",
      "----\n",
      "iter 88000, loss: 40.440418\n",
      "----\n",
      " e upiespaited to had land\n",
      "(by lettciused this with ectlong of the\n",
      "\n",
      "Un or tent on elenberg-tm the\n",
      "\n",
      "Rong tolment ten Project Gutenbing Projedaboration the firge confars\n",
      "clay Projick put pomplient, lafut \n",
      "----\n",
      "iter 89000, loss: 40.877055\n",
      "----\n",
      " ingt prived bodking worrs's satger, I'd foud you letd, I'll?\n",
      " fors then but was slowlesster't as no dising bedon like to be pay expelled any Gregor's make have tentl to sags if his fat int. I' was so; \n",
      "----\n",
      "iter 90000, loss: 39.091662\n",
      "----\n",
      " ted the furinging. He she from his wore, the stainnes leffure that then's over the couls atthe he day he diftentionide fatherseache eake it his fale of he sound make his vidietly desmet, and he sond m \n",
      "----\n",
      "iter 91000, loss: 37.880310\n",
      "----\n",
      " onr. He was sitlove had continer of equitmeally siffort crfoll lettere; unaiturem, had nod hemperted comper, and into the they - me their his certire so whet mather the reft rooks of Gregor's had the  \n",
      "----\n",
      "iter 92000, loss: 37.835810\n",
      "----\n",
      " lagg't was to armither Gregor pusher the agfation to nive!\" not and dathing mother refore poteabded ulle to the havely in, shis's father doom be to get lot clo elow to tended that prefeever. Gregor's  \n",
      "----\n",
      "iter 93000, loss: 39.233744\n",
      "----\n",
      " s and be non could.\n",
      "\n",
      "That be tore bess assrationted by ully betintmention from, atrance all and was the not comperce consly pabling to ges was the Uno  noudd yeseres on. Thempelf pittely beim, and it  \n",
      "----\n",
      "iter 94000, loss: 41.529744\n",
      "----\n",
      " blstrownound any, thetriin alled frolly owle expeccally of to is his expever hurl accuandy his father betmetth libated unaitecthonk his diserbation and then?\", back of haakssary, then? get onepy when  \n",
      "----\n",
      "iter 95000, loss: 39.824687\n",
      "----\n",
      " f the she al Gregor itting buds., Had had behs side eBfienay best terlongs betmen had beghe sides, who had nothfrees over thing they remermone himsenter him there and on eres iut the then he canclepun \n",
      "----\n",
      "iter 96000, loss: 38.323141\n",
      "----\n",
      " wastow living light in would s.isass to simen some was byil towthrectiotly coulds to bore father mouth barly very tro-uricouf and winber he had the doind in the lonce reace huper he way came playing G \n",
      "----\n",
      "iter 97000, loss: 38.068559\n",
      "----\n",
      " ege cound and he had not up sain, in his mating leftet, thair to to to ally out what he would not would have some would sud was told he would have to her mother pushed, full that mule utechenigest was \n",
      "----\n",
      "iter 98000, loss: 37.795808\n",
      "----\n",
      " d unally badiall throulfenponicud the sudring, veitgentled her was slen as be that me stently sicking huar that the deisering the\n",
      " he been to the chind. I lacks immousise began in a dny never, now and \n",
      "----\n",
      "iter 99000, loss: 39.972313\n",
      "----\n",
      "  of caund uleng it, Voniond atains of the first insoving, wist ott or gippa saicterss, butly\n",
      "mardirts tomoundy hwand firut thres work uwnald the fo'gass\n",
      "with the indeablentafforister, copy oncemborrye \n",
      "----\n",
      "iter 100000, loss: 40.464181\n",
      "----\n",
      " ng fordey band open be tin the kinf it a ginef, oussist that geady had bolly and thim trood, shilise his hom.\n",
      "If little not betod whoor that't Gregor on! Itece in why caitsing fremown't father can tha \n",
      "----\n"
     ]
    }
   ],
   "source": [
    "n, p = 0, 0\n",
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad                                                                                                                \n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0                                                                                                                        \n",
    "while n<=1000*100:\n",
    " # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "  # check \"How to feed the loss function to see how this part works\n",
    "    if p+seq_length+1 >= len(data) or n == 0:\n",
    "        hprev = np.zeros((hidden_size,1)) # reset RNN memory                                                                                                                                      \n",
    "        p = 0 # go from start of data                                                                                                                                                             \n",
    "    inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "    targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "\n",
    "  # forward seq_length characters through the net and fetch gradient                                                                                                                          \n",
    "    loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)\n",
    "    smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "\n",
    "  # sample from the model now and then                                                                                                                                                        \n",
    "    if n % 1000 == 0:\n",
    "        print ('iter %d, loss: %f' % (n, smooth_loss)) # print progress\n",
    "        sample(hprev, inputs[0], 200)\n",
    "\n",
    "  # perform parameter update with Adagrad                                                                                                                                                     \n",
    "    for param, dparam, mem in zip([Wxh, Whh, Why, bh, by],\n",
    "                                [dWxh, dWhh, dWhy, dbh, dby],\n",
    "                                [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "        \n",
    "        mem += dparam * dparam\n",
    "        param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update                                                                                                                   \n",
    "\n",
    "    p += seq_length # move data pointer                                                                                                                                                         \n",
    "    n += 1 # iteration counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
